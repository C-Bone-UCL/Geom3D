{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/rds/general/user/ma11115/home/Geom3D/Geom3D/')\n",
    "#os.chdir('C:/Users/ma11115/OneDrive - Imperial College London/Geom3d/Geom3D/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config loaded from /rds/general/user/ma11115/home/Geom3D/Geom3D/training/SchNet_Trans_80K\n",
      "loading dataset from /rds/general/user/ma11115/home/Geom3D/Geom3D/training/SchNet_Trans_80K_frag_transf_6/dataset_frag.pt\n",
      "loading model from checkpoint\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20231201_095032-4jwpllwo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/azzouzi_lab/Geom3D_fragencoding/runs/4jwpllwo' target=\"_blank\">SchNet_Trans_80K_frag_transf_6</a></strong> to <a href='https://wandb.ai/azzouzi_lab/Geom3D_fragencoding' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/azzouzi_lab/Geom3D_fragencoding' target=\"_blank\">https://wandb.ai/azzouzi_lab/Geom3D_fragencoding</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/azzouzi_lab/Geom3D_fragencoding/runs/4jwpllwo' target=\"_blank\">https://wandb.ai/azzouzi_lab/Geom3D_fragencoding/runs/4jwpllwo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n",
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:634: Checkpoint directory /rds/general/user/ma11115/home/Geom3D/Geom3D/training/SchNet_Trans_80K_frag_transf_6 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]\n",
      "\n",
      "  | Name                | Type               | Params\n",
      "-----------------------------------------------------------\n",
      "0 | input_net           | Sequential         | 98.4 K\n",
      "1 | positional_encoding | PositionalEncoding | 0     \n",
      "2 | transformer         | TransformerEncoder | 132 K \n",
      "3 | output_net          | Sequential         | 33.3 K\n",
      "4 | model_encoder       | SchNet             | 483 K \n",
      "-----------------------------------------------------------\n",
      "747 K     Trainable params\n",
      "0         Non-trainable params\n",
      "747 K     Total params\n",
      "2.992     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95319864e3b34aaf8be895bbe1ac9922",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4daae95c6344294be6694ff2d544e4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bf1202af7fe40f68e7ba0b958012d5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 3013. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2868. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2888. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2986. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2739. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2764. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2869. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2843. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2775. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2862. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2883. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2959. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2940. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2746. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2809. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2874. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 3009. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2791. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2920. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2913. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2983. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2847. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd387e7a29454106a53c7dedb4daf1de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2801. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2835. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2971. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2963. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 3026. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2859. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2837. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2761. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2845. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2976. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2742. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2782. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2770. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2981. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eb22da6460340879b55c1ef78acc087",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 3050. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2703. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 3044. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2989. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2752. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2729. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2819. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2756. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 3010. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 3085. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 3003. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08b65345a6b64fcabe0e48687f4113b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2719. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2720. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 3004. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 3079. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 3066. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2788. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 3017. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2777. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2972. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 3059. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a29c831b4cde4f60a4b12c88f844b1a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 3011. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2805. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 3034. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2992. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2965. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2709. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2786. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2806. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2998. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2781. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2990. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2872. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 3025. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2708. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91ea3d201cdc4c50bad2425afd0f1a89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2765. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 3012. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2681. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2728. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2821. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2784. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29683b5ac5284f25bf9864e6b968933f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2772. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2749. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2999. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2758. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2727. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2688. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2733. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 3005. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 3058. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/rds/general/user/ma11115/home/anaconda3/envs/Geom3D/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ea37ba03c7649c187ad03ea3e504b6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='17.327 MB of 17.327 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁███████████████████████████████████████</td></tr><tr><td>train_loss</td><td>▄▂▃▁▃▄▂▄▅▃▇▅▆█▁▃▁▃▂▃▁▂▄▂▂▁▁▂▄▂▄▁▄▃▃▆▃▅▇▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>▃▆█▁▃▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>7</td></tr><tr><td>lr-Adam</td><td>0.00049</td></tr><tr><td>train_loss</td><td>0.05075</td></tr><tr><td>trainer/global_step</td><td>3822</td></tr><tr><td>val_loss</td><td>0.04697</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">SchNet_Trans_80K_frag_transf_6</strong> at: <a href='https://wandb.ai/azzouzi_lab/Geom3D_fragencoding/runs/4jwpllwo' target=\"_blank\">https://wandb.ai/azzouzi_lab/Geom3D_fragencoding/runs/4jwpllwo</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231201_095032-4jwpllwo/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "os.chdir('/rds/general/user/ma11115/home/Geom3D/Geom3D/')\n",
    "\n",
    "%run src/geom3d/frag_encoding_with_transformer.py --config_dir /training/SchNet_Trans_80K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config loaded from training/SchNet_target_1K_TEST_5e4lr\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('/rds/general/user/ma11115/home/Geom3D/Geom3D/')\n",
    "config_dir = \"training/SchNet_target_1K_TEST_5e4lr\"\n",
    "from geom3d import frag_encoding_with_transformer \n",
    "import importlib\n",
    "import numpy as np\n",
    "import torch\n",
    "importlib.reload(frag_encoding_with_transformer)\n",
    "config = frag_encoding_with_transformer.read_config(config_dir)\n",
    "os.chdir(config[\"running_dir\"])\n",
    "np.random.seed(config[\"seed\"])\n",
    "torch.cuda.manual_seed_all(config[\"seed\"])\n",
    "config[\"device\"] = (\n",
    "    \"cuda\" if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, model = frag_encoding_with_transformer.load_data(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "def train_val_test_split(dataset, config, smiles_list=None):\n",
    "    seed = config[\"seed\"]\n",
    "    num_mols = len(dataset)\n",
    "    np.random.seed(seed)\n",
    "    all_idx = np.random.permutation(num_mols)\n",
    "    print(num_mols)\n",
    "    Nmols = num_mols\n",
    "    Ntrain = int(num_mols * config[\"train_ratio\"])\n",
    "    Nvalid = int(num_mols * config[\"valid_ratio\"])\n",
    "    Ntest = Nmols - (Ntrain + Nvalid)\n",
    "    \n",
    "    train_idx = all_idx[:Ntrain]\n",
    "    valid_idx = all_idx[Ntrain : Ntrain + Nvalid]\n",
    "    test_idx = all_idx[Ntrain + Nvalid :]\n",
    "\n",
    "    #print(\"train_idx: \", train_idx)\n",
    "    #print(\"valid_idx: \", valid_idx)\n",
    "    #print(\"test_idx: \", test_idx)\n",
    "    # np.savez(\"customized_01\", train_idx=train_idx, valid_idx=valid_idx, test_idx=test_idx)\n",
    "\n",
    "    assert len(set(train_idx).intersection(set(valid_idx))) == 0\n",
    "    assert len(set(valid_idx).intersection(set(test_idx))) == 0\n",
    "    assert len(train_idx) + len(valid_idx) + len(test_idx) == num_mols\n",
    "    train_dataset = [dataset[x] for x in train_idx]\n",
    "    valid_dataset = [dataset[x] for x in valid_idx]\n",
    "    test_dataset = [dataset[x] for x in test_idx]\n",
    "    # Set dataloaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        shuffle=True,\n",
    "        num_workers=config[\"num_workers\"],\n",
    "        drop_last=True,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        shuffle=True,\n",
    "        num_workers=config[\"num_workers\"],\n",
    "        drop_last=True,\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        shuffle=True,\n",
    "        num_workers=config[\"num_workers\"],\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    if not smiles_list:\n",
    "        return train_loader, val_loader, test_loader\n",
    "    else:\n",
    "        train_smiles = [smiles_list[i] for i in train_idx]\n",
    "        valid_smiles = [smiles_list[i] for i in valid_idx]\n",
    "        test_smiles = [smiles_list[i] for i in test_idx]\n",
    "        return (\n",
    "            train_loader,\n",
    "            val_loader,\n",
    "            test_loader,\n",
    "            (train_smiles, valid_smiles, test_smiles),\n",
    "        )\n",
    "train_loader, val_loader, test_loader = train_val_test_split(\n",
    "        dataset, config=config\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "DataBatch(x=[17112], y=[128, 128], position=[17112, 3], InChIKey=[128], batch=[17112], ptr=[129])\n",
      "DataBatch(x=[16759], y=[128, 128], position=[16759, 3], InChIKey=[128], batch=[16759], ptr=[129])\n",
      "DataBatch(x=[16635], y=[128, 128], position=[16635, 3], InChIKey=[128], batch=[16635], ptr=[129])\n",
      "DataBatch(x=[16719], y=[128, 128], position=[16719, 3], InChIKey=[128], batch=[16719], ptr=[129])\n",
      "DataBatch(x=[17161], y=[128, 128], position=[17161, 3], InChIKey=[128], batch=[17161], ptr=[129])\n",
      "DataBatch(x=[16748], y=[128, 128], position=[16748, 3], InChIKey=[128], batch=[16748], ptr=[129])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_loader, val_loader, test_loader = train_val_test_split(\n",
    "        dataset, config=config\n",
    "    )\n",
    "for batch in train_loader:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Pymodel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ma11115\\OneDrive - Imperial College London\\Geom3d\\Geom3D\\notebook\\frag_encoding_training.ipynb Cell 7\u001b[0m line \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/ma11115/OneDrive%20-%20Imperial%20College%20London/Geom3d/Geom3D/notebook/frag_encoding_training.ipynb#X24sZmlsZQ%3D%3D?line=92'>93</a>\u001b[0m \u001b[39m# check if model is in the path\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/ma11115/OneDrive%20-%20Imperial%20College%20London/Geom3d/Geom3D/notebook/frag_encoding_training.ipynb#X24sZmlsZQ%3D%3D?line=93'>94</a>\u001b[0m \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(config[\u001b[39m\"\u001b[39m\u001b[39mmodel_embedding_chkpt\u001b[39m\u001b[39m\"\u001b[39m]):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ma11115/OneDrive%20-%20Imperial%20College%20London/Geom3d/Geom3D/notebook/frag_encoding_training.ipynb#X24sZmlsZQ%3D%3D?line=94'>95</a>\u001b[0m     pymodel \u001b[39m=\u001b[39m Pymodel\u001b[39m.\u001b[39mload_from_checkpoint(config[\u001b[39m\"\u001b[39m\u001b[39mmodel_embedding_chkpt\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/ma11115/OneDrive%20-%20Imperial%20College%20London/Geom3d/Geom3D/notebook/frag_encoding_training.ipynb#X24sZmlsZQ%3D%3D?line=95'>96</a>\u001b[0m     pymodel\u001b[39m.\u001b[39mfreeze()\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/ma11115/OneDrive%20-%20Imperial%20College%20London/Geom3d/Geom3D/notebook/frag_encoding_training.ipynb#X24sZmlsZQ%3D%3D?line=96'>97</a>\u001b[0m     pymodel\u001b[39m.\u001b[39mto(config[\u001b[39m\"\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Pymodel' is not defined"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from geom3d import Database_utils\n",
    "import stk\n",
    "from torch_geometric.data import Data, Batch\n",
    "import pymongo\n",
    "\n",
    "def generate_dataset_frag(\n",
    "    df_total, model, db, number_of_molecules=1000, number_of_fragement=6\n",
    "):\n",
    "    molecule_index = np.random.choice(\n",
    "        len(df_total), number_of_molecules, replace=False\n",
    "    )\n",
    "    data_list = []\n",
    "    for i in molecule_index:\n",
    "        moldata = fragment_based_encoding(\n",
    "            df_total[\"InChIKey\"][i], db, model, number_of_fragement\n",
    "        )\n",
    "        if moldata is not None:\n",
    "            data_list.append(moldata)\n",
    "    return data_list\n",
    "\n",
    "\n",
    "def fragment_based_encoding(InChIKey, db_poly, model, number_of_fragement=6):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    polymer = db_poly.get({\"InChIKey\": InChIKey})\n",
    "    frags = []\n",
    "    dat_list = list(polymer.get_atomic_positions())\n",
    "    positions = np.vstack(dat_list)\n",
    "    positions = torch.tensor(positions, dtype=torch.float, device=device)\n",
    "    atom_types = list(\n",
    "        [\n",
    "            atom.get_atom().get_atomic_number()\n",
    "            for atom in polymer.get_atom_infos()\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    atom_types = torch.tensor(atom_types, dtype=torch.long, device=device)\n",
    "    molecule = Data(x=atom_types, positions=positions, device=device)\n",
    "    if len(list(polymer.get_building_blocks())) == number_of_fragement:\n",
    "        for molecule_bb in polymer.get_building_blocks():\n",
    "            dat_list = list(molecule_bb.get_atomic_positions())\n",
    "            positions = np.vstack(dat_list)\n",
    "            positions = torch.tensor(\n",
    "                positions, dtype=torch.float, device=device\n",
    "            )\n",
    "            atom_types = list(\n",
    "                [atom.get_atomic_number() for atom in molecule_bb.get_atoms()]\n",
    "            )\n",
    "            atom_types = torch.tensor(\n",
    "                atom_types, dtype=torch.long, device=device\n",
    "            )\n",
    "            molecule_frag = Data(\n",
    "                x=atom_types,\n",
    "                positions=positions,\n",
    "                device=device,\n",
    "            )\n",
    "            frags.append(molecule_frag)\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "\n",
    "            opt_geom_encoding = model(molecule.x, molecule.positions)\n",
    "        frag_batch = Batch.from_data_list(frags).to(device)\n",
    "\n",
    "        \"\"\"\n",
    "        model.train()\n",
    "        original_encoding = model(batch.x, batch.positions, batch.batch)\n",
    "        original_encoding = original_encoding.reshape((-1,))\n",
    "        original_encoding = original_encoding.unsqueeze(0)\n",
    "        \"\"\"\n",
    "        return frags\n",
    "        \"\"\"Data(\n",
    "            x=frag_batch.x,\n",
    "            position=frag_batch.positions,\n",
    "            y=opt_geom_encoding,\n",
    "            InChIKey=InChIKey,\n",
    "        )\"\"\"\n",
    "df_path = Path(\n",
    "        config[\"STK_path\"], \"data/output/Full_dataset/\", config[\"df_total\"]\n",
    "    )\n",
    "df_precursors_path = Path(\n",
    "    config[\"STK_path\"],\n",
    "    \"data/output/Prescursor_data/\",\n",
    "    config[\"df_precursor\"],\n",
    ")\n",
    "df_total, df_precursors = Database_utils.load_data_from_file(\n",
    "    df_path, df_precursors_path\n",
    ")\n",
    "client = pymongo.MongoClient(config[\"pymongo_client\"])\n",
    "db = stk.ConstructedMoleculeMongoDb(\n",
    "    client,\n",
    "    database=config[\"database_name\"],\n",
    ")\n",
    "# check if model is in the path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geom3d.test_train import Pymodel\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "\n",
    "def fragment_based_encoding(InChIKey, db_poly, model, number_of_fragement=6):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    polymer = db_poly.get({\"InChIKey\": InChIKey})\n",
    "    frags = []\n",
    "    dat_list = list(polymer.get_atomic_positions())\n",
    "    positions = np.vstack(dat_list)\n",
    "    positions = torch.tensor(positions, dtype=torch.float, device=device)\n",
    "    atom_types = list(\n",
    "        [\n",
    "            atom.get_atom().get_atomic_number()\n",
    "            for atom in polymer.get_atom_infos()\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    atom_types = torch.tensor(atom_types, dtype=torch.long, device=device)\n",
    "    molecule = Data(x=atom_types, positions=positions, device=device)\n",
    "    if len(list(polymer.get_building_blocks())) == number_of_fragement:\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "\n",
    "            opt_geom_encoding = model(molecule.x, molecule.positions)\n",
    "        for molecule_bb in polymer.get_building_blocks():\n",
    "            dat_list = list(molecule_bb.get_atomic_positions())\n",
    "            positions = np.vstack(dat_list)\n",
    "            positions = torch.tensor(\n",
    "                positions, dtype=torch.float, device=device\n",
    "            )\n",
    "            atom_types = list(\n",
    "                [atom.get_atomic_number() for atom in molecule_bb.get_atoms()]\n",
    "            )\n",
    "            atom_types = torch.tensor(\n",
    "                atom_types, dtype=torch.long, device=device\n",
    "            )\n",
    "            molecule_frag = Data(\n",
    "                x=atom_types,\n",
    "                positions=positions,\n",
    "                device=device,\n",
    "                y=opt_geom_encoding,\n",
    "            )\n",
    "            frags.append(molecule_frag)\n",
    "        return frags\n",
    "if os.path.exists(config[\"model_embedding_chkpt\"]):\n",
    "    pymodel = Pymodel.load_from_checkpoint(config[\"model_embedding_chkpt\"])\n",
    "    pymodel.freeze()\n",
    "    pymodel.to(config[\"device\"])\n",
    "    model = pymodel.molecule_3D_repr\n",
    "    dataset_test = generate_dataset_frag(\n",
    "        df_total,\n",
    "        model,\n",
    "        db,\n",
    "        number_of_molecules=10,#config[\"num_molecules\"],\n",
    "        number_of_fragement=config[\"number_of_fragement\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SchNet(hidden_channels=128, num_filters=128, num_interactions=6, num_gaussians=51, cutoff=10)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DataBatch(x=[243], y=[10, 128], positions=[243, 3], device=[10], batch=[243], ptr=[11]), DataBatch(x=[256], y=[10, 128], positions=[256, 3], device=[10], batch=[256], ptr=[11]), DataBatch(x=[245], y=[10, 128], positions=[245, 3], device=[10], batch=[245], ptr=[11]), DataBatch(x=[243], y=[10, 128], positions=[243, 3], device=[10], batch=[243], ptr=[11]), DataBatch(x=[231], y=[10, 128], positions=[231, 3], device=[10], batch=[231], ptr=[11]), DataBatch(x=[227], y=[10, 128], positions=[227, 3], device=[10], batch=[227], ptr=[11])]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 768])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader = DataLoader(\n",
    "        dataset_test,\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        shuffle=True,\n",
    "        num_workers=config[\"num_workers\"],\n",
    "    )\n",
    "for batch in train_loader:\n",
    "    print(batch)\n",
    "    break\n",
    "from torch_scatter import scatter_mean\n",
    "\n",
    "# Assuming `batch` is your Batch object and `x` is your node feature matrix\n",
    "x = []\n",
    "model.eval()\n",
    "for b in batch:\n",
    "   \n",
    "    x.append(model(b.x, b.positions, b.batch))\n",
    "x = torch.cat(x, dim=1)\n",
    "x.shape\n",
    "#x = torch.stack(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 128])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0].y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DataBatch(x=[233], positions=[233, 3], device=[10], batch=[233], ptr=[11]),\n",
       " DataBatch(x=[219], positions=[219, 3], device=[10], batch=[219], ptr=[11]),\n",
       " DataBatch(x=[217], positions=[217, 3], device=[10], batch=[217], ptr=[11]),\n",
       " DataBatch(x=[179], positions=[179, 3], device=[10], batch=[179], ptr=[11]),\n",
       " DataBatch(x=[214], positions=[214, 3], device=[10], batch=[214], ptr=[11]),\n",
       " DataBatch(x=[184], positions=[184, 3], device=[10], batch=[184], ptr=[11])]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "dataset_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "DataBatch(x=[16860], y=[128, 128], position=[16860, 3], InChIKey=[128], batch=[16860], ptr=[129])\n",
      "DataBatch(x=[16633], y=[128, 128], position=[16633, 3], InChIKey=[128], batch=[16633], ptr=[129])\n",
      "DataBatch(x=[17087], y=[128, 128], position=[17087, 3], InChIKey=[128], batch=[17087], ptr=[129])\n",
      "DataBatch(x=[17216], y=[128, 128], position=[17216, 3], InChIKey=[128], batch=[17216], ptr=[129])\n",
      "DataBatch(x=[16628], y=[128, 128], position=[16628, 3], InChIKey=[128], batch=[16628], ptr=[129])\n",
      "DataBatch(x=[16709], y=[128, 128], position=[16709, 3], InChIKey=[128], batch=[16709], ptr=[129])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_loader, val_loader, test_loader = train_val_test_split(\n",
    "        dataset, config=config\n",
    "    )\n",
    "for batch in train_loader:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -2.6658,  -2.1261,  -0.4586],\n",
       "        [ -6.7867,  -2.0419,  -0.9417],\n",
       "        [  5.8567,  -2.1487,  -0.8478],\n",
       "        [ -4.7182,   1.9140,   0.8504],\n",
       "        [  7.7914,   2.1979,  -0.1168],\n",
       "        [ -0.5954,   2.0266,   0.8094],\n",
       "        [-10.1401,  -4.6775,   1.3370],\n",
       "        [  2.5223,   4.3572,  -1.2846],\n",
       "        [ -9.9178,   4.1813,  -0.7012],\n",
       "        [  2.6604,  -4.4030,   1.1155],\n",
       "        [-12.1385,  -3.0203,   1.0286],\n",
       "        [  0.5018,   2.7667,  -0.9561],\n",
       "        [ -7.8972,   2.5514,  -0.4412],\n",
       "        [  4.6855,  -2.8291,   0.7903],\n",
       "        [ 12.4837,  -2.1108,  -0.1885],\n",
       "        [  9.8876,  -2.1993,  -0.1237],\n",
       "        [ 14.4257,   0.2521,  -0.5258],\n",
       "        [-13.3016,  -0.3587,   0.4650],\n",
       "        [ -0.8890,  -1.1173,  -0.3050],\n",
       "        [ -5.0217,  -1.1424,  -0.4858],\n",
       "        [  7.5833,  -1.1078,  -0.6438],\n",
       "        [ 12.1381,   2.6452,  -0.7796],\n",
       "        [ -2.3622,   1.0857,   0.4402],\n",
       "        [  9.9239,   2.5913,  -0.7352],\n",
       "        [ -6.5021,   0.9635,   0.5204],\n",
       "        [  6.0655,   1.1067,  -0.0949],\n",
       "        [ 11.0147,   3.3414,  -0.8467],\n",
       "        [-11.5121,   2.4080,  -0.2397],\n",
       "        [ -8.5244,  -2.9385,   0.8081],\n",
       "        [  4.1150,   2.6055,  -0.6930],\n",
       "        [  1.0801,  -2.6558,   0.4971],\n",
       "        [ -2.1994,  -1.1747,  -0.2502],\n",
       "        [ -6.3357,  -1.1784,  -0.4737],\n",
       "        [  6.2791,  -1.1951,  -0.5681],\n",
       "        [ 11.7814,   1.3877,  -0.6185],\n",
       "        [ -2.9658,  -0.0430,   0.0586],\n",
       "        [ 10.3355,   1.3472,  -0.5868],\n",
       "        [ -7.1066,  -0.1423,   0.0743],\n",
       "        [  5.4814,  -0.0915,  -0.2153],\n",
       "        [ -5.1885,   1.0107,   0.4897],\n",
       "        [  7.3684,   1.2083,  -0.2033],\n",
       "        [ -1.0494,   1.1264,   0.4205],\n",
       "        [-10.0850,  -3.6396,   1.0738],\n",
       "        [  2.5666,   3.3276,  -0.9873],\n",
       "        [ -9.9611,   3.1337,  -0.4752],\n",
       "        [  2.6204,  -3.3764,   0.8074],\n",
       "        [-11.1001,  -2.7702,   0.9074],\n",
       "        [  1.5269,   2.4909,  -0.7900],\n",
       "        [ -8.9297,   2.2747,  -0.3423],\n",
       "        [  3.6663,  -2.5446,   0.6071],\n",
       "        [ 11.8394,  -0.9509,  -0.3283],\n",
       "        [ 10.4191,  -0.9918,  -0.3059],\n",
       "        [ 12.5474,   0.2129,  -0.4868],\n",
       "        [-11.4092,  -0.3137,   0.3432],\n",
       "        [ -4.4214,  -0.0637,   0.0239],\n",
       "        [  8.1668,   0.0743,  -0.4169],\n",
       "        [ -0.2822,   0.0418,  -0.0321],\n",
       "        [-10.7601,   0.8695,   0.0513],\n",
       "        [ -9.2402,  -1.3879,   0.4500],\n",
       "        [  3.3537,   1.0899,  -0.3442],\n",
       "        [  1.8509,  -1.1435,   0.1397],\n",
       "        [  9.6197,   0.1284,  -0.4354],\n",
       "        [-10.6520,  -1.4631,   0.5577],\n",
       "        [  1.9429,   1.1856,  -0.3909],\n",
       "        [  1.1693,   0.0460,  -0.1164],\n",
       "        [ -8.5640,  -0.2040,   0.1599],\n",
       "        [  4.0381,  -0.1037,  -0.0966],\n",
       "        [ -9.3472,   0.9479,  -0.0309],\n",
       "        [  3.2614,  -1.2426,   0.1922]])"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_xyz[0].positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-4.6716, -0.6224,  0.3939],\n",
       "        [ 4.6590,  0.5941, -0.3888],\n",
       "        [-3.2133,  1.5758, -0.0272],\n",
       "        [ 3.2279, -1.5074,  0.0161],\n",
       "        [ 0.8665, -3.1548,  0.4134],\n",
       "        [-0.7945,  3.1447, -0.4167],\n",
       "        [-2.3551, -1.8946,  0.4338],\n",
       "        [ 2.3983,  1.8480, -0.4296],\n",
       "        [-3.5453, -0.5462,  0.3099],\n",
       "        [ 3.4783,  0.5382, -0.3043],\n",
       "        [-2.7271,  0.5647,  0.0918],\n",
       "        [ 2.7237, -0.5271, -0.0972],\n",
       "        [ 0.4095, -1.3080,  0.1682],\n",
       "        [-0.4116,  1.3092, -0.1683],\n",
       "        [-0.9416, -0.9352,  0.1998],\n",
       "        [ 0.9230,  0.9145, -0.1956],\n",
       "        [-1.4039,  0.3274,  0.0418],\n",
       "        [ 1.3778, -0.3209, -0.0411],\n",
       "        [-1.9563,  1.4426,  0.1586],\n",
       "        [ 1.9403, -1.4354, -0.1579],\n",
       "        [-3.0308, -1.2896, -0.1777],\n",
       "        [ 3.0475,  1.2385,  0.1717],\n",
       "        [ 0.1865,  1.3438,  0.1628],\n",
       "        [-0.1803, -1.3261, -0.1607],\n",
       "        [-1.0614,  0.8156,  0.0900],\n",
       "        [-1.2706, -0.5359, -0.0739],\n",
       "        [ 1.0562, -0.8027, -0.0885],\n",
       "        [ 1.2690,  0.5493,  0.0755],\n",
       "        [-1.9563,  1.4426,  0.1586],\n",
       "        [ 1.9403, -1.4354, -0.1579],\n",
       "        [-3.0308, -1.2896, -0.1777],\n",
       "        [ 3.0475,  1.2385,  0.1717],\n",
       "        [ 0.1865,  1.3438,  0.1628],\n",
       "        [-0.1803, -1.3261, -0.1607],\n",
       "        [-1.0614,  0.8156,  0.0900],\n",
       "        [-1.2706, -0.5359, -0.0739],\n",
       "        [ 1.0562, -0.8027, -0.0885],\n",
       "        [ 1.2690,  0.5493,  0.0755],\n",
       "        [-4.6716, -0.6224,  0.3939],\n",
       "        [ 4.6590,  0.5941, -0.3888],\n",
       "        [-3.2133,  1.5758, -0.0272],\n",
       "        [ 3.2279, -1.5074,  0.0161],\n",
       "        [ 0.8665, -3.1548,  0.4134],\n",
       "        [-0.7945,  3.1447, -0.4167],\n",
       "        [-2.3551, -1.8946,  0.4338],\n",
       "        [ 2.3983,  1.8480, -0.4296],\n",
       "        [-3.5453, -0.5462,  0.3099],\n",
       "        [ 3.4783,  0.5382, -0.3043],\n",
       "        [-2.7271,  0.5647,  0.0918],\n",
       "        [ 2.7237, -0.5271, -0.0972],\n",
       "        [ 0.4095, -1.3080,  0.1682],\n",
       "        [-0.4116,  1.3092, -0.1683],\n",
       "        [-0.9416, -0.9352,  0.1998],\n",
       "        [ 0.9230,  0.9145, -0.1956],\n",
       "        [-1.4039,  0.3274,  0.0418],\n",
       "        [ 1.3778, -0.3209, -0.0411],\n",
       "        [-1.9563,  1.4426,  0.1586],\n",
       "        [ 1.9403, -1.4354, -0.1579],\n",
       "        [-3.0308, -1.2896, -0.1777],\n",
       "        [ 3.0475,  1.2385,  0.1717],\n",
       "        [ 0.1865,  1.3438,  0.1628],\n",
       "        [-0.1803, -1.3261, -0.1607],\n",
       "        [-1.0614,  0.8156,  0.0900],\n",
       "        [-1.2706, -0.5359, -0.0739],\n",
       "        [ 1.0562, -0.8027, -0.0885],\n",
       "        [ 1.2690,  0.5493,  0.0755],\n",
       "        [-2.8976, -0.6882,  0.0334],\n",
       "        [-2.2828,  1.9554,  0.0784],\n",
       "        [-0.9964, -3.1080, -0.0484],\n",
       "        [ 0.5219,  3.2411,  0.0591],\n",
       "        [ 1.9301, -1.5844, -0.0649],\n",
       "        [ 2.4133,  0.5024, -0.0293],\n",
       "        [ 2.8888, -0.7139, -0.0625],\n",
       "        [ 0.7604, -0.9008, -0.0313],\n",
       "        [ 1.0652,  0.4445, -0.0083],\n",
       "        [-1.5845, -0.3171,  0.0195],\n",
       "        [-1.2914,  1.0302,  0.0428],\n",
       "        [-0.5725, -1.2500, -0.0167],\n",
       "        [ 0.0456,  1.3889,  0.0282]], device='cuda:0')"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0].position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataBatch(x=[132213], y=[1000, 128], position=[132213, 3], InChIKey=[1000], batch=[132213], ptr=[1001])"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch_geometric.data import Data, Batch\n",
    "Batch.from_data_list(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[17419], y=[128, 128], position=[17419, 3], InChIKey=[128], batch=[17419], ptr=[129])\n",
      "DataBatch(x=[16978], y=[128, 128], position=[16978, 3], InChIKey=[128], batch=[16978], ptr=[129])\n",
      "DataBatch(x=[16685], y=[128, 128], position=[16685, 3], InChIKey=[128], batch=[16685], ptr=[129])\n",
      "DataBatch(x=[16709], y=[128, 128], position=[16709, 3], InChIKey=[128], batch=[16709], ptr=[129])\n",
      "DataBatch(x=[16988], y=[128, 128], position=[16988, 3], InChIKey=[128], batch=[16988], ptr=[129])\n",
      "DataBatch(x=[16623], y=[128, 128], position=[16623, 3], InChIKey=[128], batch=[16623], ptr=[129])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geom3d.transformer_utils import TransformerPredictor\n",
    "class Fragment_encoder(TransformerPredictor):\n",
    "    def add_encoder(self, model_encoder):\n",
    "        self.model_encoder = model_encoder\n",
    "\n",
    "    def forward(self, batch, mask=None, add_positional_encoding=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input features of shape [Batch, SeqLen, input_dim]\n",
    "            mask: Mask to apply on the attention outputs (optional)\n",
    "            add_positional_encoding: If True, we add the positional encoding to the input.\n",
    "                                      Might not be desired for some tasks.\n",
    "        \"\"\"\n",
    "        if self.model_encoder is not None:\n",
    "            #print(batch.shape)\n",
    "            x = self.model_encoder(batch.x, batch.position, batch.batch)\n",
    "            \n",
    "            #x = batch.reshape((-1, 1))\n",
    "        x.cuda()\n",
    "        self.input_net.cuda()\n",
    "        x = self.input_net(x)\n",
    "        if add_positional_encoding:\n",
    "            x = self.positional_encoding(x)\n",
    "        x = self.transformer(x, mask=mask)\n",
    "        x = self.output_net(x)\n",
    "        return x\n",
    "\n",
    "    def _calculate_loss(self, batch, mode=\"train\"):\n",
    "        # Fetch data and transform categories to one-hot vectors\n",
    "        inp_data, labels = batch, batch.y.squeeze()\n",
    "\n",
    "        # inp_data = F.one_hot(inp_data, num_classes=self.hparams.num_classes).float()\n",
    "\n",
    "        # Perform prediction and calculate loss and accuracy\n",
    "        preds = self.forward(inp_data, add_positional_encoding=True)\n",
    "        loss = Functional.mse_loss(preds.view(-1, preds.size(-1)), labels)\n",
    "        acc = (preds.argmax(dim=-1) == labels).float().mean()\n",
    "\n",
    "        # Logging\n",
    "        self.log(\"%s_loss\" % mode, loss)\n",
    "        return loss, acc\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, _ = self._calculate_loss(batch, mode=\"train\")\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        _ = self._calculate_loss(batch, mode=\"val\")\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        _ = self._calculate_loss(batch, mode=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "\n",
    "EncodingModel = Fragment_encoder(\n",
    "    input_dim= config[\"emb_dim\"],\n",
    "    model_dim=config[\"emb_dim\"],\n",
    "    num_heads=1,\n",
    "    num_classes=train_loader.dataset[0].y.shape[1],\n",
    "    num_layers=1,\n",
    "    dropout=0.0,\n",
    "    lr=5e-4,\n",
    "    warmup=50,\n",
    "    max_iters=config[\"max_epochs\"] * len(train_loader),\n",
    ")\n",
    "EncodingModel.add_encoder(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fragment_encoder(\n",
       "  (input_net): Sequential(\n",
       "    (0): Dropout(p=0.0, inplace=False)\n",
       "    (1): Linear(in_features=128, out_features=128, bias=True)\n",
       "  )\n",
       "  (positional_encoding): PositionalEncoding()\n",
       "  (transformer): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): EncoderBlock(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (qkv_proj): Linear(in_features=128, out_features=384, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear_net): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=256, bias=True)\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Linear(in_features=256, out_features=128, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (output_net): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.0, inplace=False)\n",
       "    (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "  )\n",
       "  (model_encoder): SchNet(hidden_channels=128, num_filters=128, num_interactions=6, num_gaussians=51, cutoff=10)\n",
       ")"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EncodingModel.cuda()\n",
    "EncodingModel.to(config[\"device\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = torch.load(\"/rds/general/user/ma11115/home/Geom3D/Geom3D/training/SchNet_Trans_80K_frag_transf_6/epoch=27-val_loss=0.10-other_metric=0.00.ckpt\")\n",
    "EncodingModel.load_state_dict(state_dict['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['input_net.1.weight', 'input_net.1.bias', 'transformer.layers.0.self_attn.qkv_proj.weight', 'transformer.layers.0.self_attn.qkv_proj.bias', 'transformer.layers.0.self_attn.o_proj.weight', 'transformer.layers.0.self_attn.o_proj.bias', 'transformer.layers.0.linear_net.0.weight', 'transformer.layers.0.linear_net.0.bias', 'transformer.layers.0.linear_net.3.weight', 'transformer.layers.0.linear_net.3.bias', 'transformer.layers.0.norm1.weight', 'transformer.layers.0.norm1.bias', 'transformer.layers.0.norm2.weight', 'transformer.layers.0.norm2.bias', 'output_net.0.weight', 'output_net.0.bias', 'output_net.1.weight', 'output_net.1.bias', 'output_net.4.weight', 'output_net.4.bias', 'model_encoder.atomic_mass', 'model_encoder.embedding.weight', 'model_encoder.distance_expansion.offset', 'model_encoder.interactions.0.mlp.0.weight', 'model_encoder.interactions.0.mlp.0.bias', 'model_encoder.interactions.0.mlp.2.weight', 'model_encoder.interactions.0.mlp.2.bias', 'model_encoder.interactions.0.conv.lin1.weight', 'model_encoder.interactions.0.conv.lin2.weight', 'model_encoder.interactions.0.conv.lin2.bias', 'model_encoder.interactions.0.conv.nn.0.weight', 'model_encoder.interactions.0.conv.nn.0.bias', 'model_encoder.interactions.0.conv.nn.2.weight', 'model_encoder.interactions.0.conv.nn.2.bias', 'model_encoder.interactions.0.lin.weight', 'model_encoder.interactions.0.lin.bias', 'model_encoder.interactions.1.mlp.0.weight', 'model_encoder.interactions.1.mlp.0.bias', 'model_encoder.interactions.1.mlp.2.weight', 'model_encoder.interactions.1.mlp.2.bias', 'model_encoder.interactions.1.conv.lin1.weight', 'model_encoder.interactions.1.conv.lin2.weight', 'model_encoder.interactions.1.conv.lin2.bias', 'model_encoder.interactions.1.conv.nn.0.weight', 'model_encoder.interactions.1.conv.nn.0.bias', 'model_encoder.interactions.1.conv.nn.2.weight', 'model_encoder.interactions.1.conv.nn.2.bias', 'model_encoder.interactions.1.lin.weight', 'model_encoder.interactions.1.lin.bias', 'model_encoder.interactions.2.mlp.0.weight', 'model_encoder.interactions.2.mlp.0.bias', 'model_encoder.interactions.2.mlp.2.weight', 'model_encoder.interactions.2.mlp.2.bias', 'model_encoder.interactions.2.conv.lin1.weight', 'model_encoder.interactions.2.conv.lin2.weight', 'model_encoder.interactions.2.conv.lin2.bias', 'model_encoder.interactions.2.conv.nn.0.weight', 'model_encoder.interactions.2.conv.nn.0.bias', 'model_encoder.interactions.2.conv.nn.2.weight', 'model_encoder.interactions.2.conv.nn.2.bias', 'model_encoder.interactions.2.lin.weight', 'model_encoder.interactions.2.lin.bias', 'model_encoder.interactions.3.mlp.0.weight', 'model_encoder.interactions.3.mlp.0.bias', 'model_encoder.interactions.3.mlp.2.weight', 'model_encoder.interactions.3.mlp.2.bias', 'model_encoder.interactions.3.conv.lin1.weight', 'model_encoder.interactions.3.conv.lin2.weight', 'model_encoder.interactions.3.conv.lin2.bias', 'model_encoder.interactions.3.conv.nn.0.weight', 'model_encoder.interactions.3.conv.nn.0.bias', 'model_encoder.interactions.3.conv.nn.2.weight', 'model_encoder.interactions.3.conv.nn.2.bias', 'model_encoder.interactions.3.lin.weight', 'model_encoder.interactions.3.lin.bias', 'model_encoder.interactions.4.mlp.0.weight', 'model_encoder.interactions.4.mlp.0.bias', 'model_encoder.interactions.4.mlp.2.weight', 'model_encoder.interactions.4.mlp.2.bias', 'model_encoder.interactions.4.conv.lin1.weight', 'model_encoder.interactions.4.conv.lin2.weight', 'model_encoder.interactions.4.conv.lin2.bias', 'model_encoder.interactions.4.conv.nn.0.weight', 'model_encoder.interactions.4.conv.nn.0.bias', 'model_encoder.interactions.4.conv.nn.2.weight', 'model_encoder.interactions.4.conv.nn.2.bias', 'model_encoder.interactions.4.lin.weight', 'model_encoder.interactions.4.lin.bias', 'model_encoder.interactions.5.mlp.0.weight', 'model_encoder.interactions.5.mlp.0.bias', 'model_encoder.interactions.5.mlp.2.weight', 'model_encoder.interactions.5.mlp.2.bias', 'model_encoder.interactions.5.conv.lin1.weight', 'model_encoder.interactions.5.conv.lin2.weight', 'model_encoder.interactions.5.conv.lin2.bias', 'model_encoder.interactions.5.conv.nn.0.weight', 'model_encoder.interactions.5.conv.nn.0.bias', 'model_encoder.interactions.5.conv.nn.2.weight', 'model_encoder.interactions.5.conv.nn.2.bias', 'model_encoder.interactions.5.lin.weight', 'model_encoder.interactions.5.lin.bias', 'model_encoder.lin1.weight', 'model_encoder.lin1.bias', 'model_encoder.lin2.weight', 'model_encoder.lin2.bias'])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict['state_dict'].keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Geom3D",
   "language": "python",
   "name": "geom3d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
